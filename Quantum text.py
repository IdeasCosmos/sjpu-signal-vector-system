"""
í•˜ì´ë¸Œë¦¬ë“œ SJPU ì‹œìŠ¤í…œ - í•µì‹¬ í´ë˜ìŠ¤ êµ¬í˜„
ì‘ì€í‹€ 1.1: ê¸°ë³¸ SJPU í´ë˜ìŠ¤ êµ¬í˜„ (80% ì™„ì„±ë„)
"""

import numpy as np
import time
import logging
import json
import hashlib
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from collections import defaultdict, deque
from enum import Enum
import threading
import psutil
import gc

# =============================================================================
# í•µì‹¬ ì„¤ì • í´ë˜ìŠ¤ (ì™„ì „ êµ¬í˜„)
# =============================================================================

class ProcessingMode(Enum):
    """ì²˜ë¦¬ ëª¨ë“œ ì—´ê±°í˜•"""
    QUANTUM = "quantum"
    HYBRID = "hybrid"
    CLASSICAL = "classical"
    AUTO = "auto"

class PerformanceLevel(Enum):
    """ì„±ëŠ¥ ë ˆë²¨"""
    ULTRA_LOW = 1    # ê·¹ì €ì‚¬ì–‘ (< 128MB)
    LOW = 2          # ì €ì‚¬ì–‘ (128-256MB)
    MEDIUM = 3       # ì¤‘ì‚¬ì–‘ (256-512MB)
    HIGH = 4         # ê³ ì‚¬ì–‘ (512MB-1GB)
    ULTRA_HIGH = 5   # ìµœê³ ì‚¬ì–‘ (> 1GB)

@dataclass
class HybridSJPUConfig:
    """í•˜ì´ë¸Œë¦¬ë“œ SJPU ì‹œìŠ¤í…œ ì™„ì „ ì„¤ì • í´ë˜ìŠ¤"""
    
    # === ê¸°ë³¸ ì°¨ì› ì„¤ì • ===
    vector_dimensions: int = 256
    semantic_dimensions: int = 512
    quantum_register_size: int = 8
    emotion_dimensions: int = 32
    
    # === ëª¨ë“œ ì „í™˜ ì„ê³„ê°’ ===
    quantum_threshold: int = 1000      # 1000ì ì´í•˜ = ì–‘ì ëª¨ë“œ
    classical_threshold: int = 5000    # 5000ì ì´ìƒ = í´ë˜ì‹ ëª¨ë“œ
    hybrid_threshold: int = 2500       # ì¤‘ê°„ = í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ
    
    # === ë©”ëª¨ë¦¬ ê´€ë¦¬ ===
    max_memory_mb: int = 512
    cache_size_limit: int = 1000
    gc_frequency: int = 100
    memory_warning_threshold: float = 0.8
    memory_critical_threshold: float = 0.9
    
    # === ì„±ëŠ¥ ìµœì í™” ===
    use_ml_predictor: bool = True
    use_quantum_cache: bool = True
    use_genetic_optimizer: bool = True
    use_multiprocessing: bool = False
    max_workers: int = 4
    
    # === ê³ ê¸‰ ê¸°ëŠ¥ í™œì„±í™” ===
    consciousness_stream: bool = True
    temporal_compression: bool = True
    quantum_compression: bool = True
    dream_state_processing: bool = True
    selective_attention: bool = True
    
    # === ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§ ===
    debug_mode: bool = False
    verbose_logging: bool = False
    performance_monitoring: bool = True
    auto_optimization: bool = True
    
    # === ë³´ì•ˆ ë° ì•ˆì •ì„± ===
    max_processing_time: float = 30.0  # ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    input_size_limit: int = 100000     # ìµœëŒ€ ì…ë ¥ í¬ê¸° (ë¬¸ì)
    output_size_limit: int = 50000     # ìµœëŒ€ ì¶œë ¥ í¬ê¸° (ë¬¸ì)
    
    def __post_init__(self):
        """ì„¤ì • ê²€ì¦ ë° ìë™ ì¡°ì •"""
        self._validate_config()
        self._auto_adjust_config()
    
    def _validate_config(self):
        """ì„¤ì •ê°’ ìœ íš¨ì„± ê²€ì‚¬"""
        if self.vector_dimensions < 16 or self.vector_dimensions > 2048:
            raise ValueError(f"vector_dimensions must be 16-2048, got {self.vector_dimensions}")
        
        if self.max_memory_mb < 64:
            raise ValueError(f"max_memory_mb must be >= 64MB, got {self.max_memory_mb}")
        
        if not 0 < self.memory_warning_threshold < self.memory_critical_threshold < 1:
            raise ValueError("Memory thresholds must be: 0 < warning < critical < 1")
    
    def _auto_adjust_config(self):
        """ì‹œìŠ¤í…œ í™˜ê²½ì— ë”°ë¥¸ ìë™ ì„¤ì • ì¡°ì •"""
        try:
            # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í™•ì¸
            total_memory = psutil.virtual_memory().total / (1024**3)  # GB
            
            if total_memory < 4:  # 4GB ë¯¸ë§Œ
                self.max_memory_mb = min(self.max_memory_mb, 256)
                self.cache_size_limit = min(self.cache_size_limit, 500)
                self.vector_dimensions = min(self.vector_dimensions, 128)
                
            elif total_memory < 8:  # 8GB ë¯¸ë§Œ
                self.max_memory_mb = min(self.max_memory_mb, 512)
                self.cache_size_limit = min(self.cache_size_limit, 1000)
                
            # CPU ì½”ì–´ ìˆ˜ì— ë”°ë¥¸ ì›Œì»¤ ì¡°ì •
            cpu_count = psutil.cpu_count()
            self.max_workers = min(self.max_workers, max(2, cpu_count - 1))
            
        except ImportError:
            # psutilì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ìœ ì§€
            pass
    
    def get_performance_level(self) -> PerformanceLevel:
        """í˜„ì¬ ì„¤ì • ê¸°ë°˜ ì„±ëŠ¥ ë ˆë²¨ ë°˜í™˜"""
        if self.max_memory_mb <= 128:
            return PerformanceLevel.ULTRA_LOW
        elif self.max_memory_mb <= 256:
            return PerformanceLevel.LOW
        elif self.max_memory_mb <= 512:
            return PerformanceLevel.MEDIUM
        elif self.max_memory_mb <= 1024:
            return PerformanceLevel.HIGH
        else:
            return PerformanceLevel.ULTRA_HIGH
    
    def optimize_for_speed(self):
        """ì†ë„ ìš°ì„  ìµœì í™”"""
        self.use_quantum_cache = True
        self.use_multiprocessing = True
        self.quantum_compression = False
        self.dream_state_processing = False
        
    def optimize_for_memory(self):
        """ë©”ëª¨ë¦¬ ìš°ì„  ìµœì í™”"""
        self.max_memory_mb = min(self.max_memory_mb, 256)
        self.cache_size_limit = min(self.cache_size_limit, 500)
        self.vector_dimensions = min(self.vector_dimensions, 128)
        self.quantum_compression = True
        self.temporal_compression = True
    
    def to_dict(self) -> Dict[str, Any]:
        """ì„¤ì •ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            field.name: getattr(self, field.name) 
            for field in self.__dataclass_fields__.values()
        }
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'HybridSJPUConfig':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ì„¤ì • ìƒì„±"""
        return cls(**{k: v for k, v in config_dict.items() 
                     if k in cls.__dataclass_fields__})

# =============================================================================
# ì²˜ë¦¬ ê²°ê³¼ í´ë˜ìŠ¤
# =============================================================================

@dataclass
class SJPUResult:
    """SJPU ì²˜ë¦¬ ê²°ê³¼"""
    original_text: str
    processed_text: str
    processing_mode: ProcessingMode
    processing_time: float
    memory_used: float
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ìŠ¤
    input_length: int = 0
    output_length: int = 0
    compression_ratio: float = 1.0
    quality_score: float = 0.5
    
    # ë©”íƒ€ë°ì´í„°
    timestamp: float = field(default_factory=time.time)
    config_hash: str = ""
    error_message: Optional[str] = None
    
    # ê³ ê¸‰ ë¶„ì„ ê²°ê³¼
    consciousness_analysis: Optional[Dict[str, Any]] = None
    quantum_coherence: Optional[List[float]] = None
    genetic_optimization: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if not self.input_length:
            self.input_length = len(self.original_text)
        if not self.output_length:
            self.output_length = len(self.processed_text)
        
        if self.input_length > 0:
            self.compression_ratio = self.output_length / self.input_length
    
    def get_efficiency_score(self) -> float:
        """íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚° (0-1)"""
        if self.processing_time <= 0:
            return 0.0
        
        # ì²˜ë¦¬ ì†ë„ ì ìˆ˜ (1000ì/ì´ˆ ê¸°ì¤€)
        speed_score = min(1.0, (self.input_length / 1000) / self.processing_time)
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì ìˆ˜ (100MB ê¸°ì¤€)
        memory_score = max(0.0, 1.0 - (self.memory_used / 100))
        
        # í’ˆì§ˆ ì ìˆ˜
        quality_score = self.quality_score
        
        return (speed_score * 0.4 + memory_score * 0.3 + quality_score * 0.3)

# =============================================================================
# ë©”ì¸ SJPU í´ë˜ìŠ¤ (ì™„ì „ êµ¬í˜„)
# =============================================================================

class PredictiveHybridSJPU:
    """ì˜ˆì¸¡ì  í•˜ì´ë¸Œë¦¬ë“œ SJPU ì‹œìŠ¤í…œ - ì™„ì „ êµ¬í˜„"""
    
    def __init__(self, config: Optional[HybridSJPUConfig] = None):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.config = config or HybridSJPUConfig()
        self._setup_logging()
        self._initialize_components()
        self._setup_monitoring()
        
        # í†µê³„ ë° ìƒíƒœ
        self.processing_stats = {
            'total_processed': 0,
            'mode_distribution': defaultdict(int),
            'total_processing_time': 0.0,
            'total_memory_used': 0.0,
            'error_count': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        self.performance_history = deque(maxlen=1000)
        self.last_gc_time = time.time()
        self._lock = threading.Lock()
        
        self.logger.info(f"SJPU System initialized with {self.config.get_performance_level().name} performance level")
    
    def _setup_logging(self):
        """ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
        level = logging.DEBUG if self.config.debug_mode else logging.INFO
        
        logging.basicConfig(
            level=level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        self.logger = logging.getLogger(f"{self.__class__.__name__}")
        
        if self.config.verbose_logging:
            self.logger.setLevel(logging.DEBUG)
    
    def _initialize_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ìºì‹œ (ê°„ë‹¨ ë²„ì „)
        self.simple_cache = {}
        self.cache_access_count = defaultdict(int)
        
        # ëª¨ë“œ ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬
        self.mode_prediction_history = deque(maxlen=100)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.last_memory_check = time.time()
        self.memory_usage_history = deque(maxlen=50)
        
        self.logger.debug("Core components initialized")
    
    def _setup_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        if self.config.performance_monitoring:
            self._start_memory_monitor()
    
    def _start_memory_monitor(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        def monitor():
            while True:
                current_memory = self._get_memory_usage()
                self.memory_usage_history.append(current_memory)
                
                if current_memory > self.config.max_memory_mb * self.config.memory_critical_threshold:
                    self.logger.warning(f"Critical memory usage: {current_memory:.1f}MB")
                    self._emergency_cleanup()
                
                time.sleep(10)  # 10ì´ˆë§ˆë‹¤ ì²´í¬
        
        if self.config.performance_monitoring:
            monitor_thread = threading.Thread(target=monitor, daemon=True)
            monitor_thread.start()
    
    def _get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
        try:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / (1024 * 1024)
        except ImportError:
            # psutil ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì¶”ì •
            return len(str(self.__dict__)) / (1024 * 10)  # ëŒ€ëµì  ì¶”ì •
    
    def adaptive_process(self, text: str, context: str = "", 
                        mode: ProcessingMode = ProcessingMode.AUTO,
                        user_preferences: Optional[Dict[str, Any]] = None) -> SJPUResult:
        """
        ì ì‘ì  í…ìŠ¤íŠ¸ ì²˜ë¦¬ - í•µì‹¬ ë©”ì„œë“œ (ì™„ì „ êµ¬í˜„)
        
        Args:
            text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            mode: ì²˜ë¦¬ ëª¨ë“œ (AUTOë©´ ìë™ ì„ íƒ)
            user_preferences: ì‚¬ìš©ì ì„ í˜¸ ì„¤ì •
        
        Returns:
            SJPUResult: ì²˜ë¦¬ ê²°ê³¼ ê°ì²´
        """
        start_time = time.time()
        
        try:
            # ì…ë ¥ ê²€ì¦
            self._validate_input(text, context)
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            current_memory = self._get_memory_usage()
            
            # ì²˜ë¦¬ ëª¨ë“œ ê²°ì •
            if mode == ProcessingMode.AUTO:
                processing_mode = self._predict_optimal_mode(text, context, current_memory)
            else:
                processing_mode = mode
            
            self.logger.debug(f"Processing mode selected: {processing_mode.value}")
            
            # ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(text, context, processing_mode)
            cached_result = self._check_cache(cache_key)
            
            if cached_result:
                self.processing_stats['cache_hits'] += 1
                return self._create_cached_result(cached_result, start_time)
            
            self.processing_stats['cache_misses'] += 1
            
            # ëª¨ë“œë³„ ì²˜ë¦¬ ì‹¤í–‰
            processed_text = self._execute_processing_mode(
                text, context, processing_mode, user_preferences
            )
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = self._evaluate_quality(text, processed_text)
            
            # ê²°ê³¼ ìƒì„±
            result = SJPUResult(
                original_text=text,
                processed_text=processed_text,
                processing_mode=processing_mode,
                processing_time=time.time() - start_time,
                memory_used=self._get_memory_usage() - current_memory,
                quality_score=quality_score,
                config_hash=self._get_config_hash()
            )
            
            # ìºì‹œ ì €ì¥
            self._store_in_cache(cache_key, processed_text, result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_statistics(result)
            
            # ìë™ ìµœì í™”
            if self.config.auto_optimization:
                self._auto_optimize(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Processing failed: {str(e)}", exc_info=True)
            self.processing_stats['error_count'] += 1
            
            return SJPUResult(
                original_text=text,
                processed_text=f"[ERROR] {text[:50]}{'...' if len(text) > 50 else ''}",
                processing_mode=ProcessingMode.CLASSICAL,
                processing_time=time.time() - start_time,
                memory_used=0,
                error_message=str(e)
            )
    
    def _validate_input(self, text: str, context: str):
        """ì…ë ¥ ê²€ì¦"""
        if not isinstance(text, str):
            raise ValueError("Text must be a string")
        
        if len(text) > self.config.input_size_limit:
            raise ValueError(f"Input too large: {len(text)} > {self.config.input_size_limit}")
        
        if len(context) > self.config.input_size_limit // 2:
            raise ValueError(f"Context too large: {len(context)} > {self.config.input_size_limit // 2}")
    
    def _predict_optimal_mode(self, text: str, context: str, current_memory: float) -> ProcessingMode:
        """ìµœì  ì²˜ë¦¬ ëª¨ë“œ ì˜ˆì¸¡"""
        text_length = len(text)
        total_length = len(text + context)
        
        # ë©”ëª¨ë¦¬ ì••ë°•ë„ ê³„ì‚°
        memory_pressure = current_memory / self.config.max_memory_mb
        
        # ë³µì¡ë„ ë¶„ì„
        complexity_score = self._analyze_text_complexity(text)
        
        # ëª¨ë“œ ê²°ì • ë¡œì§
        if memory_pressure > 0.8:
            # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ í´ë˜ì‹ ëª¨ë“œ
            selected_mode = ProcessingMode.CLASSICAL
        elif text_length < self.config.quantum_threshold and complexity_score < 0.6:
            # ì§§ê³  ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ëŠ” ì–‘ì ëª¨ë“œ
            selected_mode = ProcessingMode.QUANTUM
        elif text_length > self.config.classical_threshold or complexity_score > 0.8:
            # ê¸¸ê±°ë‚˜ ë³µì¡í•œ í…ìŠ¤íŠ¸ëŠ” í´ë˜ì‹ ëª¨ë“œ
            selected_mode = ProcessingMode.CLASSICAL
        else:
            # ì¤‘ê°„ í¬ê¸°ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ
            selected_mode = ProcessingMode.HYBRID
        
        # ì˜ˆì¸¡ íˆìŠ¤í† ë¦¬ ê¸°ë¡
        self.mode_prediction_history.append({
            'text_length': text_length,
            'complexity': complexity_score,
            'memory_pressure': memory_pressure,
            'predicted_mode': selected_mode
        })
        
        return selected_mode
    
    def _analyze_text_complexity(self, text: str) -> float:
        """í…ìŠ¤íŠ¸ ë³µì¡ë„ ë¶„ì„ (0-1)"""
        if not text:
            return 0.0
        
        words = text.split()
        if not words:
            return 0.1
        
        # ì–´íœ˜ ë‹¤ì–‘ì„±
        unique_words = len(set(word.lower() for word in words))
        vocabulary_diversity = unique_words / len(words)
        
        # í‰ê·  ë‹¨ì–´ ê¸¸ì´
        avg_word_length = np.mean([len(word) for word in words])
        length_complexity = min(1.0, avg_word_length / 10.0)
        
        # ë¬¸ì¥ ë³µì¡ë„
        sentences = text.split('.')
        avg_sentence_length = np.mean([len(s.split()) for s in sentences if s.strip()])
        sentence_complexity = min(1.0, avg_sentence_length / 20.0)
        
        # íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨
        special_chars = len([c for c in text if not c.isalnum() and not c.isspace()])
        special_ratio = special_chars / len(text)
        
        # ì¢…í•© ë³µì¡ë„
        complexity = (
            vocabulary_diversity * 0.3 +
            length_complexity * 0.3 +
            sentence_complexity * 0.3 +
            special_ratio * 0.1
        )
        
        return min(1.0, complexity)
    
    def _generate_cache_key(self, text: str, context: str, mode: ProcessingMode) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{text}|{context}|{mode.value}|{self.config.vector_dimensions}"
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def _check_cache(self, cache_key: str) -> Optional[str]:
        """ìºì‹œ í™•ì¸"""
        if not self.config.use_quantum_cache:
            return None
        
        if cache_key in self.simple_cache:
            self.cache_access_count[cache_key] += 1
            return self.simple_cache[cache_key]
        
        return None
    
    def _execute_processing_mode(self, text: str, context: str, 
                                mode: ProcessingMode,
                                user_preferences: Optional[Dict[str, Any]]) -> str:
        """ëª¨ë“œë³„ ì²˜ë¦¬ ì‹¤í–‰"""
        
        if mode == ProcessingMode.QUANTUM:
            return self._quantum_process(text, context, user_preferences)
        elif mode == ProcessingMode.HYBRID:
            return self._hybrid_process(text, context, user_preferences)
        else:  # CLASSICAL
            return self._classical_process(text, context, user_preferences)
    
    def _quantum_process(self, text: str, context: str, 
                        user_preferences: Optional[Dict[str, Any]]) -> str:
        """ì–‘ì ëª¨ë“œ ì²˜ë¦¬ (80% êµ¬í˜„)"""
        self.logger.debug("Executing quantum processing")
        
        words = text.split()
        if not words:
            return text
        
        # ì–‘ì ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜
        quantum_amplitudes = []
        for i, word in enumerate(words):
            # ë‹¨ì–´ì˜ í•´ì‹œë¥¼ ì´ìš©í•œ ì–‘ì ìƒíƒœ ìƒì„±
            word_hash = hash(word.lower()) % 1000
            amplitude = (word_hash / 1000.0) * 0.8 + 0.1  # 0.1-0.9 ë²”ìœ„
            phase = (word_hash % 360) * np.pi / 180
            
            quantum_state = amplitude * np.exp(1j * phase)
            quantum_amplitudes.append(abs(quantum_state))
        
        # ë†’ì€ ì§„í­ì˜ ë‹¨ì–´ë“¤ ì„ íƒ (ì–‘ì ì¸¡ì • ì‹œë®¬ë ˆì´ì…˜)
        word_importance = list(zip(words, quantum_amplitudes, range(len(words))))
        word_importance.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ 70% ë‹¨ì–´ ì„ íƒ
        num_selected = max(3, int(len(words) * 0.7))
        selected_words = word_importance[:num_selected]
        
        # ì›ë˜ ìˆœì„œë¡œ ì¬ì •ë ¬
        selected_words.sort(key=lambda x: x[2])
        
        # ì–‘ì ì–½í˜ íš¨ê³¼ ì‹œë®¬ë ˆì´ì…˜ (ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì—°ê²°)
        result_words = []
        for i, (word, amplitude, _) in enumerate(selected_words):
            result_words.append(word)
            
            # ë†’ì€ ì§„í­ì˜ ë‹¨ì–´ì— ì–‘ì ì–½í˜ í‘œì‹œ ì¶”ê°€
            if amplitude > 0.8 and i < len(selected_words) - 1:
                if self.config.consciousness_stream:
                    result_words.append("âŸ¨âŸ©")  # ì–‘ì ì–½í˜ í‘œì‹œ
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë³´ì •
        if context:
            result_text = " ".join(result_words)
            context_words = set(context.lower().split())
            result_text_words = result_text.lower().split()
            
            # ì»¨í…ìŠ¤íŠ¸ì™€ ê´€ë ¨ëœ ë‹¨ì–´ ê°•ì¡°
            enhanced_words = []
            for word in result_words:
                if word.lower() in context_words:
                    enhanced_words.append(f"*{word}*")  # ê°•ì¡° í‘œì‹œ
                else:
                    enhanced_words.append(word)
            
            return " ".join(enhanced_words)
        
        return " ".join(result_words)
    
    def _hybrid_process(self, text: str, context: str,
                       user_preferences: Optional[Dict[str, Any]]) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì²˜ë¦¬"""
        self.logger.debug("Executing hybrid processing")
        
        # í…ìŠ¤íŠ¸ë¥¼ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë¶„í• 
        mid_point = len(text) // 2
        
        # ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
        words = text.split()
        mid_word = len(words) // 2
        
        part1_words = words[:mid_word]
        part2_words = words[mid_word:]
        
        part1_text = " ".join(part1_words)
        part2_text = " ".join(part2_words)
        
        # ì²« ë¶€ë¶„ì€ ì–‘ìë¡œ, ë‘ ë²ˆì§¸ ë¶€ë¶„ì€ í´ë˜ì‹ìœ¼ë¡œ
        quantum_result = self._quantum_process(part1_text, context, user_preferences)
        classical_result = self._classical_process(part2_text, context, user_preferences)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì—°ê²°
        connector = " âŠ• " if self.config.consciousness_stream else " | "
        
        return f"{quantum_result}{connector}{classical_result}"
    
    def _classical_process(self, text: str, context: str,
                          user_preferences: Optional[Dict[str, Any]]) -> str:
        """í´ë˜ì‹ ëª¨ë“œ ì²˜ë¦¬"""
        self.logger.debug("Executing classical processing")
        
        words = text.split()
        if not words:
            return text
        
        # ì „í†µì ì¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (í‚¤ì›Œë“œ ì¶”ì¶œ + ìš”ì•½)
        if len(words) <= 5:
            # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ
            return text
        elif len(words) <= 20:
            # ì¤‘ê°„ ê¸¸ì´ëŠ” í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
            # ê¸¸ì´ê°€ ê¸´ ë‹¨ì–´ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
            word_scores = [(word, len(word), i) for i, word in enumerate(words)]
            word_scores.sort(key=lambda x: (x[1], -x[2]), reverse=True)  # ê¸¸ì´ìˆœ, ì•ìˆœì„œ ìš°ì„ 
            
            selected_count = max(3, len(words) // 2)
            selected = word_scores[:selected_count]
            selected.sort(key=lambda x: x[2])  # ì›ë˜ ìˆœì„œë¡œ ë³µì›
            
            result = [item[0] for item in selected]
            return " ".join(result)
        else:
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìš”ì•½
            sentences = text.split('.')
            if len(sentences) <= 2:
                return text
            
            # ì¤‘ê°„ ë¬¸ì¥ë“¤ ì¤‘ ê°€ì¥ ê¸´ ê²ƒë“¤ ì„ íƒ
            sentence_scores = [(s.strip(), len(s.split()), i) 
                             for i, s in enumerate(sentences) if s.strip()]
            sentence_scores.sort(key=lambda x: x[1], reverse=True)
            
            selected_count = max(1, len(sentence_scores) // 2)
            selected_sentences = sentence_scores[:selected_count]
            selected_sentences.sort(key=lambda x: x[2])  # ì›ë˜ ìˆœì„œ
            
            result = [item[0] for item in selected_sentences]
            return ". ".join(result) + "."
    
    def _evaluate_quality(self, original_text: str, processed_text: str) -> float:
        """í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ (0-1)"""
        if not original_text or not processed_text:
            return 0.0
        
        original_words = set(original_text.lower().split())
        processed_words = set(processed_text.lower().split())
        
        if not original_words:
            return 0.0
        
        # í•µì‹¬ ë‹¨ì–´ ë³´ì¡´ìœ¨
        preserved_ratio = len(original_words & processed_words) / len(original_words)
        
        # ê¸¸ì´ ì ì •ì„± (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
        length_ratio = len(processed_text) / len(original_text)
        length_score = 1.0 - abs(0.7 - length_ratio)  # 70% ê¸¸ì´ë¥¼ ìµœì ìœ¼ë¡œ ê°€ì •
        length_score = max(0.0, min(1.0, length_score))
        
        # ê°€ë…ì„± (ë‹¨ì–´ ìˆ˜ì™€ ë¬¸ì¥ êµ¬ì¡°)
        processed_sentences = processed_text.split('.')
        readability_score = min(1.0, len(processed_sentences) / max(1, len(processed_sentences)))
        
        # ì¢…í•© í’ˆì§ˆ ì ìˆ˜
        quality_score = (preserved_ratio * 0.5 + length_score * 0.3 + readability_score * 0.2)
        
        return min(1.0, max(0.0, quality_score))
    
    def _store_in_cache(self, cache_key: str, processed_text: str, result: SJPUResult):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        if not self.config.use_quantum_cache:
            return
        
        # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
        if len(self.simple_cache) >= self.config.cache_size_limit:
            self._evict_cache()
        
        self.simple_cache[cache_key] = processed_text
        self.cache_access_count[cache_key] = 1
    
    def _evict_cache(self):
        """ìºì‹œ ì œê±° (LRU ê¸°ë°˜)"""
        if not self.simple_cache:
            return
        
        # ì ‘ê·¼ íšŸìˆ˜ê°€ ê°€ì¥ ì ì€ í•­ëª© ì œê±°
        least_accessed = min(self.cache_access_count.items(), key=lambda x: x[1])
        key_to_remove = least_accessed[0]
        
        self.simple_cache.pop(key_to_remove, None)
        self.cache_access_count.pop(key_to_remove, None)
        
        self.logger.debug(f"Cache evicted: {key_to_remove}")
    
    def _create_cached_result(self, cached_text: str, start_time: float) -> SJPUResult:
        """ìºì‹œëœ ê²°ê³¼ ìƒì„±"""
        return SJPUResult(
            original_text="[CACHED]",
            processed_text=cached_text,
            processing_mode=ProcessingMode.QUANTUM,  # ìºì‹œëŠ” ì–‘ì ëª¨ë“œë¡œ ê°„ì£¼
            processing_time=time.time() - start_time,
            memory_used=0.0,  # ìºì‹œëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš© ì—†ìŒ
            quality_score=0.9  # ìºì‹œëŠ” ë†’ì€ í’ˆì§ˆë¡œ ê°€ì •
        )
    
    def _update_statistics(self, result: SJPUResult):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        with self._lock:
            self.processing_stats['total_processed'] += 1
            self.processing_stats['mode_distribution'][result.processing_mode.value] += 1
            self.processing_stats['total_processing_time'] += result.processing_time
            self.processing_stats['total_memory_used'] += result.memory_used
            
            # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì¶”ê°€
            self.performance_history.append({
                'timestamp': result.timestamp,
                'mode': result.processing_mode.value,
                'processing_time': result.processing_time,
                'memory_used': result.memory_used,
                'quality_score': result.quality_score,
                'efficiency_score': result.get_efficiency_score()
            })
    
    def _get_config_hash(self) -> str:
        """í˜„ì¬ ì„¤ì • í•´ì‹œ"""
        config_str = json.dumps(self.config.to_dict(), sort_keys=True)
        return hashlib.md5(config_str.encode()).hexdigest()[:8]
    
    def _auto_optimize(self, result: SJPUResult):
        """ìë™ ìµœì í™”"""
        # ì„±ëŠ¥ì´ ë‚®ìœ¼ë©´ ì„¤ì • ì¡°ì •
        if result.get_efficiency_score() < 0.5:
            if result.memory_used > self.config.max_memory_mb * 0.7:
                self.config.optimize_for_memory()
                self.logger.info("Auto-optimized for memory")
            elif result.processing_time > 5.0:
                self.config.optimize_for_speed()
                self.logger.info("Auto-optimized for speed")
    
    def _emergency_cleanup(self):
        """ê¸´ê¸‰ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        self.logger.warning("Emergency cleanup initiated")
        
        # ìºì‹œ 50% ì œê±°
        cache_items = list(self.simple_cache.items())
        items_to_remove = len(cache_items) // 2
        
        for key, _ in cache_items[:items_to_remove]:
            self.simple_cache.pop(key, None)
            self.cache_access_count.pop(key, None)
        
        # íˆìŠ¤í† ë¦¬ ì œê±°
        while len(self.performance_history) > 100:
            self.performance_history.popleft()
        
        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        self.logger.info("Emergency cleanup completed")
    
    # =============================================================================
    # ê³µê°œ API ë©”ì„œë“œë“¤
    # =============================================================================
    
    def process(self, text: str, context: str = "") -> SJPUResult:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ - ê³µê°œ API"""
        return self.adaptive_process(text, context)
    
    def process_quantum(self, text: str, context: str = "") -> SJPUResult:
        """ì–‘ì ëª¨ë“œ ê°•ì œ ì²˜ë¦¬"""
        return self.adaptive_process(text, context, ProcessingMode.QUANTUM)
    
    def process_classical(self, text: str, context: str = "") -> SJPUResult:
        """í´ë˜ì‹ ëª¨ë“œ ê°•ì œ ì²˜ë¦¬"""
        return self.adaptive_process(text, context, ProcessingMode.CLASSICAL)
    
    def batch_process(self, texts: List[str], contexts: List[str] = None) -> List[SJPUResult]:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        if contexts is None:
            contexts = [""] * len(texts)
        elif len(contexts) != len(texts):
            raise ValueError("Texts and contexts must have same length")
        
        results = []
        for text, context in zip(texts, contexts):
            try:
                result = self.adaptive_process(text, context)
                results.append(result)
            except Exception as e:
                self.logger.error(f"Batch item failed: {e}")
                error_result = SJPUResult(
                    original_text=text,
                    processed_text=f"[ERROR] {text[:30]}...",
                    processing_mode=ProcessingMode.CLASSICAL,
                    processing_time=0.001,
                    memory_used=0,
                    error_message=str(e)
                )
                results.append(error_result)
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜"""
        with self._lock:
            total_processed = self.processing_stats['total_processed']
            
            stats = {
                'total_processed': total_processed,
                'mode_distribution': dict(self.processing_stats['mode_distribution']),
                'average_processing_time': (
                    self.processing_stats['total_processing_time'] / max(total_processed, 1)
                ),
                'average_memory_usage': (
                    self.processing_stats['total_memory_used'] / max(total_processed, 1)
                ),
                'error_rate': (
                    self.processing_stats['error_count'] / max(total_processed, 1)
                ),
                'cache_hit_rate': (
                    self.processing_stats['cache_hits'] / 
                    max(self.processing_stats['cache_hits'] + self.processing_stats['cache_misses'], 1)
                ),
                'cache_size': len(self.simple_cache),
                'performance_level': self.config.get_performance_level().name,
                'current_memory_usage': self._get_memory_usage()
            }
            
            # ìµœê·¼ ì„±ëŠ¥ í†µê³„
            if self.performance_history:
                recent_performance = list(self.performance_history)[-10:]
                stats['recent_avg_efficiency'] = np.mean([
                    p['efficiency_score'] for p in recent_performance
                ])
                stats['recent_avg_quality'] = np.mean([
                    p['quality_score'] for p in recent_performance
                ])
            
            return stats
    
    def optimize_system(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìµœì í™” ì‹¤í–‰"""
        start_time = time.time()
        optimizations_applied = []
        
        initial_memory = self._get_memory_usage()
        
        # 1. ìºì‹œ ìµœì í™”
        if len(self.simple_cache) > self.config.cache_size_limit * 0.8:
            old_size = len(self.simple_cache)
            self._evict_cache()
            new_size = len(self.simple_cache)
            optimizations_applied.append(f"Cache optimized: {old_size} â†’ {new_size}")
        
        # 2. ë©”ëª¨ë¦¬ ì •ë¦¬
        if initial_memory > self.config.max_memory_mb * 0.7:
            gc.collect()
            optimizations_applied.append("Memory garbage collected")
        
        # 3. íˆìŠ¤í† ë¦¬ ì •ë¦¬
        if len(self.performance_history) > 800:
            while len(self.performance_history) > 500:
                self.performance_history.popleft()
            optimizations_applied.append("Performance history trimmed")
        
        # 4. ì„¤ì • ìë™ ì¡°ì •
        if self.performance_history:
            recent_efficiency = np.mean([
                p['efficiency_score'] for p in list(self.performance_history)[-20:]
            ])
            if recent_efficiency < 0.6:
                self.config.optimize_for_speed()
                optimizations_applied.append("Config optimized for speed")
        
        final_memory = self._get_memory_usage()
        optimization_time = time.time() - start_time
        
        return {
            'optimization_time': optimization_time,
            'optimizations_applied': optimizations_applied,
            'memory_saved_mb': initial_memory - final_memory,
            'initial_memory_mb': initial_memory,
            'final_memory_mb': final_memory
        }
    
    def clear_cache(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        self.simple_cache.clear()
        self.cache_access_count.clear()
        self.logger.info("Cache cleared")
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        with self._lock:
            self.processing_stats = {
                'total_processed': 0,
                'mode_distribution': defaultdict(int),
                'total_processing_time': 0.0,
                'total_memory_used': 0.0,
                'error_count': 0,
                'cache_hits': 0,
                'cache_misses': 0
            }
            self.performance_history.clear()
        
        self.logger.info("Statistics reset")
    
    def shutdown(self):
        """ì‹œìŠ¤í…œ ì¢…ë£Œ"""
        self.logger.info("SJPU System shutting down...")
        
        # ìºì‹œ ì €ì¥ (í•„ìš”ì‹œ)
        if self.config.debug_mode:
            cache_stats = {
                'cache_size': len(self.simple_cache),
                'total_access': sum(self.cache_access_count.values())
            }
            self.logger.debug(f"Final cache stats: {cache_stats}")
        
        # ì •ë¦¬
        self.clear_cache()
        self.reset_stats()
        
        self.logger.info("SJPU System shutdown completed")

# =============================================================================
# ê°„í¸ ì‚¬ìš©ì„ ìœ„í•œ ë˜í¼ í´ë˜ìŠ¤
# =============================================================================

class SJPU:
    """ê°„í¸ ì‚¬ìš©ì„ ìœ„í•œ SJPU ë˜í¼ í´ë˜ìŠ¤"""
    
    def __init__(self, performance_level: str = "medium", debug: bool = False):
        """
        ê°„ë‹¨í•œ ì´ˆê¸°í™”
        
        Args:
            performance_level: "ultra_low", "low", "medium", "high", "ultra_high"
            debug: ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”
        """
        config = HybridSJPUConfig(debug_mode=debug, verbose_logging=debug)
        
        # ì„±ëŠ¥ ë ˆë²¨ë³„ ì„¤ì •
        if performance_level.lower() == "ultra_low":
            config.max_memory_mb = 128
            config.vector_dimensions = 64
            config.cache_size_limit = 200
        elif performance_level.lower() == "low":
            config.max_memory_mb = 256
            config.vector_dimensions = 128
            config.cache_size_limit = 500
        elif performance_level.lower() == "high":
            config.max_memory_mb = 1024
            config.vector_dimensions = 512
            config.cache_size_limit = 2000
        elif performance_level.lower() == "ultra_high":
            config.max_memory_mb = 2048
            config.vector_dimensions = 1024
            config.cache_size_limit = 5000
        # mediumì€ ê¸°ë³¸ê°’ ì‚¬ìš©
        
        self.engine = PredictiveHybridSJPU(config)
    
    def process(self, text: str, context: str = "") -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ - ê²°ê³¼ ë¬¸ìì—´ë§Œ ë°˜í™˜"""
        result = self.engine.process(text, context)
        return result.processed_text
    
    def process_detailed(self, text: str, context: str = "") -> SJPUResult:
        """ìƒì„¸í•œ ì²˜ë¦¬ ê²°ê³¼ ë°˜í™˜"""
        return self.engine.process(text, context)
    
    def stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„"""
        return self.engine.get_stats()
    
    def optimize(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìµœì í™”"""
        return self.engine.optimize_system()

# =============================================================================
# ì‚¬ìš© ì˜ˆì œ ë° í…ŒìŠ¤íŠ¸
# =============================================================================

def run_basic_tests():
    """ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª SJPU ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ê°„ë‹¨í•œ ì‚¬ìš©ë²•
    sjpu = SJPU(performance_level="medium", debug=True)
    
    test_cases = [
        ("ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.", "ì¼ìƒ ëŒ€í™”"),
        ("ì¸ê³µì§€ëŠ¥ì˜ ë°œì „ì€ ì¸ë¥˜ì—ê²Œ ë§ì€ ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ê²ƒì…ë‹ˆë‹¤. " * 5, "ê¸°ìˆ  ë…¼ì˜"),
        ("ì§§ì€ í…ìŠ¤íŠ¸", ""),
        ("ì´ê²ƒì€ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 20, "ê¸´ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸")
    ]
    
    for i, (text, context) in enumerate(test_cases, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: '{text[:30]}{'...' if len(text) > 30 else ''}'")
        
        try:
            # ìƒì„¸ ê²°ê³¼
            result = sjpu.process_detailed(text, context)
            
            print(f"   ëª¨ë“œ: {result.processing_mode.value}")
            print(f"   ì²˜ë¦¬ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
            print(f"   ë©”ëª¨ë¦¬: {result.memory_used:.1f}MB")
            print(f"   í’ˆì§ˆì ìˆ˜: {result.quality_score:.2f}")
            print(f"   íš¨ìœ¨ì„±: {result.get_efficiency_score():.2f}")
            print(f"   ê²°ê³¼: {result.processed_text[:50]}{'...' if len(result.processed_text) > 50 else ''}")
            print("   âœ… ì„±ê³µ")
            
        except Exception as e:
            print(f"   âŒ ì‹¤íŒ¨: {e}")
    
    # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print(f"\nğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    batch_texts = [case[0] for case in test_cases[:2]]
    batch_contexts = [case[1] for case in test_cases[:2]]
    
    batch_results = sjpu.engine.batch_process(batch_texts, batch_contexts)
    print(f"   ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(batch_results)}/{len(batch_texts)}")
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì‹œìŠ¤í…œ í†µê³„:")
    stats = sjpu.stats()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"   {key}: {value:.3f}")
        else:
            print(f"   {key}: {value}")
    
    print(f"\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    run_basic_tests()